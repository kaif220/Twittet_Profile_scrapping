{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to twitter_profiles.csv\n"
     ]
    }
   ],
   "source": [
    "#this is scrap by selenium library \n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--window-size=1920x1080\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "input_file = \"twitter_links.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "output_file = \"twitter_profiles.csv\"\n",
    "columns = [\"Twitter Link\", \"Bio\", \"Following Count\", \"Followers Count\", \"Location\", \"Website\"]\n",
    "output_data = []\n",
    "\n",
    "def scrape_twitter_profile(url):\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    def get_text(xpath):\n",
    "        try:\n",
    "            element = wait.until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            return element.text.strip()\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "    \n",
    "    def get_href(xpath):\n",
    "        try:\n",
    "            element = wait.until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "            return element.get_attribute(\"href\")\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "    \n",
    "    bio = get_text(\"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/div/div[1]/div/div[3]/div/div/span\")\n",
    "    following_count = get_text(\"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/div/div[1]/div/div[5]/div[1]/a/span[1]/span\")\n",
    "    followers_count = get_text(\"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/div/div[1]/div/div[5]/div[2]/a/span[1]/span\")\n",
    "    location = get_text(\"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/div/div[1]/div/div[4]/div/span[1]/span/span\")\n",
    "    website = get_href(\"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/div/div[1]/div/div[4]/div/a/span\")\n",
    "    \n",
    "    return [url, bio, following_count, followers_count, location, website]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    twitter_link = row.iloc[0]\n",
    "    try:\n",
    "        profile_data = scrape_twitter_profile(twitter_link)\n",
    "        output_data.append(profile_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {twitter_link}: {e}\")\n",
    "        output_data.append([twitter_link, \"Error\", \"Error\", \"Error\", \"Error\", \"Error\"])\n",
    "\n",
    "output_df = pd.DataFrame(output_data, columns=columns)\n",
    "output_df.to_csv(output_file, index=False)\n",
    "\n",
    "driver.quit()\n",
    "print(f\"Scraped data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is code i use selenium in this code i scrapped data in this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links: 15\n",
      "Scraped link: 1\n",
      "Scraped link: 2\n",
      "Scraped link: 3\n",
      "Scraped link: 4\n",
      "Scraped link: 5\n",
      "Scraped link: 6\n",
      "Scraped link: 7\n",
      "Scraped link: 8\n",
      "Invalid Profile Link. Skipping link: 9\n",
      "Scraped link: 10\n",
      "Scraped link: 11\n",
      "Invalid Profile Link. Skipping link: 12\n",
      "Scraped link: 13\n",
      "Scraped link: 14\n",
      "Scraped link: 15\n",
      "Scraped data saved to profile.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Declaring constants\n",
    "USER_EMAIL = \"deathstorm226@gmail.com\"\n",
    "USERNAME = \"@deathstorm226\"\n",
    "USER_PASSWORD = \"Mohd@78600\"\n",
    "\n",
    "# Loading the profile links from the CSV File\n",
    "links = []\n",
    "with open('twitter_links.csv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        links.append(row[0])\n",
    "print(f\"Total number of links: {len(links)}\")\n",
    "\n",
    "# Initializing the Web Driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://twitter.com/i/flow/login\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Login process\n",
    "input_field = driver.find_element(By.XPATH, \"//input\")\n",
    "input_field.send_keys(USER_EMAIL)\n",
    "input_field.send_keys(Keys.ENTER)\n",
    "time.sleep(10)\n",
    "\n",
    "try:\n",
    "    username_field = driver.find_element(By.XPATH, \"//input[@data-testid='ocfEnterTextTextInput']\")\n",
    "    username_field.send_keys(USERNAME)\n",
    "    username_field.send_keys(Keys.ENTER)\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "pwd_field = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, \"//input[@name='password']\"))\n",
    ")\n",
    "pwd_field.send_keys(USER_PASSWORD)\n",
    "pwd_field.send_keys(Keys.ENTER)\n",
    "time.sleep(10)\n",
    "\n",
    "dict_list = []\n",
    "\n",
    "# Looping through each link\n",
    "for i, link in enumerate(links):\n",
    "    driver.get(link)\n",
    "    time.sleep(5)\n",
    "\n",
    "    try:\n",
    "        username_div = driver.find_element(By.XPATH, \"//*[contains(@data-testid, 'UserName')]\")\n",
    "    except:\n",
    "        print(f\"Invalid Profile Link. Skipping link: {i+1}\")\n",
    "        continue\n",
    "    username_span = username_div.find_element(By.XPATH, \".//span\")\n",
    "    username = username_span.text\n",
    "\n",
    "    # Fetching bio if exists.\n",
    "    try:\n",
    "        bio_div = driver.find_element(By.XPATH, \"//div[@data-testid='UserDescription']\")\n",
    "        bio_span = bio_div.find_element(By.XPATH, \".//span\")\n",
    "        bio = bio_span.text\n",
    "    except:\n",
    "        bio = None\n",
    "\n",
    "    # Fetching website if exists.\n",
    "    try:\n",
    "        website_anchor = driver.find_element(By.XPATH, \"//a[@data-testid='UserUrl']\")\n",
    "        website = website_anchor.get_attribute(\"href\")\n",
    "    except:\n",
    "        website = None\n",
    "\n",
    "    # Fetching location if exists.\n",
    "    try:\n",
    "        location_span = driver.find_element(By.XPATH, \"//span[@data-testid='UserLocation']\")\n",
    "        location = location_span.text\n",
    "    except:\n",
    "        location = None\n",
    "\n",
    "    # Fetching following\n",
    "    try:\n",
    "        following_anchor = driver.find_elements(By.XPATH, \"//a[contains(@href, 'following')]\")[0]\n",
    "        following = following_anchor.text\n",
    "    except:\n",
    "        following = None\n",
    "\n",
    "    # Fetching followers\n",
    "    try:\n",
    "        follower_anchor = driver.find_elements(By.XPATH, \"//a[contains(@href, 'followers')]\")[0]\n",
    "        follower = follower_anchor.text\n",
    "    except:\n",
    "        follower = None\n",
    "\n",
    "    # Fetching the website URL from bio if present\n",
    "    try:\n",
    "        website_span = driver.find_element(By.XPATH, \"//a[@data-testid='UserUrl']\")\n",
    "        website = website_span.get_attribute(\"href\")\n",
    "    except:\n",
    "        website = None\n",
    "\n",
    "    # Appending profile details to the list\n",
    "    profile_dict = {\n",
    "        \"Username\": username,\n",
    "        \"Bio\": bio,\n",
    "        \"Location\": location,\n",
    "        \"Following\": following,\n",
    "        \"Follower\": follower,\n",
    "        \"Website\": website\n",
    "    }\n",
    "    dict_list.append(profile_dict)\n",
    "    print(f\"Scraped link: {i+1}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "csv_filename = \"profile.csv\"\n",
    "\n",
    "# Dumping the list of dictionaries to CSV file\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=dict_list[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dict_list)\n",
    "\n",
    "print(f\"Scraped data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
